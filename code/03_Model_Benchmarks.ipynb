{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "import requests\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from bs4 import BeautifulSoup             \n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/train_combine.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "subreddit        0\n",
       "combine_text     0\n",
       "clean_combine    6\n",
       "is_coffee        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>combine_text</th>\n",
       "      <th>clean_combine</th>\n",
       "      <th>is_coffee</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Coffee</td>\n",
       "      <td>\\n\\nWelcome to the daily [/r/Coffee](https://...</td>\n",
       "      <td>welcome daily thread stupid ask answer start s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Coffee</td>\n",
       "      <td>Welcome to the /r/Coffee deal and promotional ...</td>\n",
       "      <td>welcome deal promotional thread weekly thread ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Coffee</td>\n",
       "      <td>Had a barista look at me like o was an idiot a...</td>\n",
       "      <td>barista look idiot ask flat white armed exact ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Coffee</td>\n",
       "      <td>Hello everyone, I’m new to this sub so please ...</td>\n",
       "      <td>hello everyone new sub please direct topic fit...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Coffee</td>\n",
       "      <td>I've been getting pre-ground coffee (Bustelo/L...</td>\n",
       "      <td>pre ground bustelo lavazza moka pot recently s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1906</th>\n",
       "      <td>tea</td>\n",
       "      <td>Brand new to Tea! Has anyone tried Vahdam tea...</td>\n",
       "      <td>brand new anyone vahdam matcha</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1907</th>\n",
       "      <td>tea</td>\n",
       "      <td>A nice, relaxing day with a Dan Cong oolong a...</td>\n",
       "      <td>nice relaxing day dan cong oolong secret spot</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1908</th>\n",
       "      <td>tea</td>\n",
       "      <td>Clouds over a cup when tea is the sun.</td>\n",
       "      <td>cloud sun</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1909</th>\n",
       "      <td>tea</td>\n",
       "      <td>Benefits? of getting loose leaf at a bar.</td>\n",
       "      <td>benefit loose leaf bar</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1910</th>\n",
       "      <td>tea</td>\n",
       "      <td>Just came across a great podcast on the histo...</td>\n",
       "      <td>come across great podcast history china</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1905 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     subreddit                                       combine_text  \\\n",
       "0       Coffee   \\n\\nWelcome to the daily [/r/Coffee](https://...   \n",
       "1       Coffee  Welcome to the /r/Coffee deal and promotional ...   \n",
       "2       Coffee  Had a barista look at me like o was an idiot a...   \n",
       "3       Coffee  Hello everyone, I’m new to this sub so please ...   \n",
       "4       Coffee  I've been getting pre-ground coffee (Bustelo/L...   \n",
       "...        ...                                                ...   \n",
       "1906       tea   Brand new to Tea! Has anyone tried Vahdam tea...   \n",
       "1907       tea   A nice, relaxing day with a Dan Cong oolong a...   \n",
       "1908       tea             Clouds over a cup when tea is the sun.   \n",
       "1909       tea          Benefits? of getting loose leaf at a bar.   \n",
       "1910       tea   Just came across a great podcast on the histo...   \n",
       "\n",
       "                                          clean_combine  is_coffee  \n",
       "0     welcome daily thread stupid ask answer start s...          1  \n",
       "1     welcome deal promotional thread weekly thread ...          1  \n",
       "2     barista look idiot ask flat white armed exact ...          1  \n",
       "3     hello everyone new sub please direct topic fit...          1  \n",
       "4     pre ground bustelo lavazza moka pot recently s...          1  \n",
       "...                                                 ...        ...  \n",
       "1906                     brand new anyone vahdam matcha          0  \n",
       "1907      nice relaxing day dan cong oolong secret spot          0  \n",
       "1908                                          cloud sun          0  \n",
       "1909                             benefit loose leaf bar          0  \n",
       "1910            come across great podcast history china          0  \n",
       "\n",
       "[1905 rows x 4 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define X, y and check baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define X, y \n",
    "X = df['clean_combine']\n",
    "y = df['is_coffee']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.514436\n",
       "1    0.485564\n",
       "Name: is_coffee, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train/test/split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train_test_split.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size = 0.25,\n",
    "                                                    random_state = 42,\n",
    "                                                    stratify = y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use CountVectorizer for Logisctic regression and Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvec = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_features = cvec.fit_transform(X_train)\n",
    "\n",
    "test_data_features =cvec.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9943977591036415"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate logistic regression model.\n",
    "lr = LogisticRegression()\n",
    "\n",
    "# Fit model to training data.\n",
    "lr.fit(train_data_features, y_train)\n",
    "\n",
    "# Evaluate model on training data.\n",
    "lr.score(train_data_features, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.89937106918239"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.score(test_data_features, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9607843137254902"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit model to training data.\n",
    "nb.fit(train_data_features, y_train)\n",
    "\n",
    "# Evaluate model on training data.\n",
    "nb.score(train_data_features, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9203354297693921"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb.score(test_data_features, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use TfidfVectorizer for Logisctic regression and Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tvec = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_features = tvec.fit_transform(X_train)\n",
    "\n",
    "test_data_features =tvec.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9754901960784313"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate logistic regression model.\n",
    "lr = LogisticRegression()\n",
    "\n",
    "# Fit model to training data.\n",
    "lr.fit(train_data_features, y_train)\n",
    "\n",
    "# Evaluate model on training data.\n",
    "lr.score(train_data_features, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.909853249475891"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.score(test_data_features, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9705882352941176"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb.fit(train_data_features, y_train)\n",
    "\n",
    "# Evaluate model on training data.\n",
    "nb.score(train_data_features, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9224318658280922"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb.score(test_data_features, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use Pipeline for CountVectorizer Logisctic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe1 = Pipeline([\n",
    "    ('cvec', CountVectorizer()),\n",
    "    ('lr', LogisticRegression(max_iter=200)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('cvec', CountVectorizer()),\n",
       "  ('lr', LogisticRegression(max_iter=200))],\n",
       " 'verbose': False,\n",
       " 'cvec': CountVectorizer(),\n",
       " 'lr': LogisticRegression(max_iter=200),\n",
       " 'cvec__analyzer': 'word',\n",
       " 'cvec__binary': False,\n",
       " 'cvec__decode_error': 'strict',\n",
       " 'cvec__dtype': numpy.int64,\n",
       " 'cvec__encoding': 'utf-8',\n",
       " 'cvec__input': 'content',\n",
       " 'cvec__lowercase': True,\n",
       " 'cvec__max_df': 1.0,\n",
       " 'cvec__max_features': None,\n",
       " 'cvec__min_df': 1,\n",
       " 'cvec__ngram_range': (1, 1),\n",
       " 'cvec__preprocessor': None,\n",
       " 'cvec__stop_words': None,\n",
       " 'cvec__strip_accents': None,\n",
       " 'cvec__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'cvec__tokenizer': None,\n",
       " 'cvec__vocabulary': None,\n",
       " 'lr__C': 1.0,\n",
       " 'lr__class_weight': None,\n",
       " 'lr__dual': False,\n",
       " 'lr__fit_intercept': True,\n",
       " 'lr__intercept_scaling': 1,\n",
       " 'lr__l1_ratio': None,\n",
       " 'lr__max_iter': 200,\n",
       " 'lr__multi_class': 'auto',\n",
       " 'lr__n_jobs': None,\n",
       " 'lr__penalty': 'l2',\n",
       " 'lr__random_state': None,\n",
       " 'lr__solver': 'lbfgs',\n",
       " 'lr__tol': 0.0001,\n",
       " 'lr__verbose': 0,\n",
       " 'lr__warm_start': False}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe1.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_params1 = {\n",
    "    'cvec__max_features': [1000, 2000, 3000],\n",
    "    'cvec__min_df': [2,3,4,],\n",
    "    'cvec__max_df': [0.9, 0.95],\n",
    "    'cvec__ngram_range': [(1,1), (1,2)],\n",
    "    #'lr__C': [100, 10, 1.0, 0.1],\n",
    "    'lr__C': [1.0, 0.1, 0.01],\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs1 = GridSearchCV(pipe1, # what object are we optimizing?\n",
    "                  param_grid= pipe_params1, # what parameters values are we searching?\n",
    "                  cv=5) # 5-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('cvec', CountVectorizer()),\n",
       "                                       ('lr',\n",
       "                                        LogisticRegression(max_iter=200))]),\n",
       "             param_grid={'cvec__max_df': [0.9, 0.95],\n",
       "                         'cvec__max_features': [1000, 2000, 3000],\n",
       "                         'cvec__min_df': [2, 3, 4],\n",
       "                         'cvec__ngram_range': [(1, 1), (1, 2)],\n",
       "                         'lr__C': [1.0, 0.1, 0.01]})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs1.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9082591093117409"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs1.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs1_model =gs1.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.992296918767507"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs1_model.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9077568134171907"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs1_model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('cvec',\n",
       "                 CountVectorizer(max_df=0.9, max_features=3000, min_df=2,\n",
       "                                 ngram_range=(1, 2))),\n",
       "                ('lr', LogisticRegression(max_iter=200))])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs1_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Comparing to default C (C=1) for Logistic regression, best C value for this model is 10. High C value in logistic regression has weaker regularization strength. So we see higher overfitting in this model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use Pipeline for TfidfVectorizer Logisctic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe2 = Pipeline([\n",
    "    ('tvec', TfidfVectorizer()),\n",
    "    ('lr', LogisticRegression(max_iter = 200)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('tvec', TfidfVectorizer()),\n",
       "  ('lr', LogisticRegression(max_iter=200))],\n",
       " 'verbose': False,\n",
       " 'tvec': TfidfVectorizer(),\n",
       " 'lr': LogisticRegression(max_iter=200),\n",
       " 'tvec__analyzer': 'word',\n",
       " 'tvec__binary': False,\n",
       " 'tvec__decode_error': 'strict',\n",
       " 'tvec__dtype': numpy.float64,\n",
       " 'tvec__encoding': 'utf-8',\n",
       " 'tvec__input': 'content',\n",
       " 'tvec__lowercase': True,\n",
       " 'tvec__max_df': 1.0,\n",
       " 'tvec__max_features': None,\n",
       " 'tvec__min_df': 1,\n",
       " 'tvec__ngram_range': (1, 1),\n",
       " 'tvec__norm': 'l2',\n",
       " 'tvec__preprocessor': None,\n",
       " 'tvec__smooth_idf': True,\n",
       " 'tvec__stop_words': None,\n",
       " 'tvec__strip_accents': None,\n",
       " 'tvec__sublinear_tf': False,\n",
       " 'tvec__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'tvec__tokenizer': None,\n",
       " 'tvec__use_idf': True,\n",
       " 'tvec__vocabulary': None,\n",
       " 'lr__C': 1.0,\n",
       " 'lr__class_weight': None,\n",
       " 'lr__dual': False,\n",
       " 'lr__fit_intercept': True,\n",
       " 'lr__intercept_scaling': 1,\n",
       " 'lr__l1_ratio': None,\n",
       " 'lr__max_iter': 200,\n",
       " 'lr__multi_class': 'auto',\n",
       " 'lr__n_jobs': None,\n",
       " 'lr__penalty': 'l2',\n",
       " 'lr__random_state': None,\n",
       " 'lr__solver': 'lbfgs',\n",
       " 'lr__tol': 0.0001,\n",
       " 'lr__verbose': 0,\n",
       " 'lr__warm_start': False}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe2.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_params2 = {\n",
    "    'tvec__max_features': [1000, 2000, 3000],\n",
    "    'tvec__min_df': [2,3,4],\n",
    "    'tvec__max_df': [0.9, 0.95],\n",
    "    'tvec__ngram_range': [(1,1), (1,2)],\n",
    "    #'lr__C': [100, 10, 1.0, 0.1],\n",
    "    'lr__C': [1.0, 0.1, 0.01],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs2 = GridSearchCV(pipe2, # what object are we optimizing?\n",
    "                  param_grid= pipe_params2, # what parameters values are we searching?\n",
    "                  cv=5) # 5-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('tvec', TfidfVectorizer()),\n",
       "                                       ('lr',\n",
       "                                        LogisticRegression(max_iter=200))]),\n",
       "             param_grid={'lr__C': [1.0, 0.1, 0.01], 'tvec__max_df': [0.9, 0.95],\n",
       "                         'tvec__max_features': [1000, 2000, 3000],\n",
       "                         'tvec__min_df': [2, 3, 4],\n",
       "                         'tvec__ngram_range': [(1, 1), (1, 2)]})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs2.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9152643847380689"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs2.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs2_model=gs2.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9747899159663865"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs2_model.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9203354297693921"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs2_model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tvec',\n",
       "                 TfidfVectorizer(max_df=0.9, max_features=2000, min_df=2)),\n",
       "                ('lr', LogisticRegression(max_iter=200))])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs2_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use Pipeline for CountVectorizer Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe3 = Pipeline([\n",
    "    ('cvec', CountVectorizer()),\n",
    "    ('nb', MultinomialNB()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_params3 = {\n",
    "    'cvec__max_features': [1000, 2000, 3000],\n",
    "    'cvec__min_df': [2,3,4],\n",
    "    'cvec__max_df': [0.9, 0.95],\n",
    "    'cvec__ngram_range': [(1,1), (1,2)],\n",
    "    #'nb__alpha': [0.1, 1, 10, 100]\n",
    "    'nb__alpha': [1, 10, 100]\n",
    "    #'nb__alpha' : np.linspace(0.1, 1.0, 20),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_params3_1 = {\n",
    "    'cvec__max_features': [100, 200, 300],\n",
    "    'cvec__min_df': [2,3,4],\n",
    "    'cvec__max_df': [0.9, 0.95],\n",
    "    'cvec__ngram_range': [(1,1), (1,2)],\n",
    "    'nb__alpha': [0.1, 1, 10, 100]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs3 = GridSearchCV(pipe3, # what object are we optimizing?\n",
    "                  param_grid= pipe_params3, # what parameters values are we searching?\n",
    "                  cv=5) # 5-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs3_1 = GridSearchCV(pipe3, # what object are we optimizing?\n",
    "                  param_grid= pipe_params3_1, # what parameters values are we searching?\n",
    "                  cv=5) # 5-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('cvec', CountVectorizer()),\n",
       "                                       ('nb', MultinomialNB())]),\n",
       "             param_grid={'cvec__max_df': [0.9, 0.95],\n",
       "                         'cvec__max_features': [1000, 2000, 3000],\n",
       "                         'cvec__min_df': [2, 3, 4],\n",
       "                         'cvec__ngram_range': [(1, 1), (1, 2)],\n",
       "                         'nb__alpha': [1, 10, 100]})"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs3.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9019654030180344"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs3.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs3_model =gs3.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9565826330532213"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs3_model.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9329140461215933"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs3_model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('cvec',\n",
       "                 CountVectorizer(max_df=0.9, max_features=3000, min_df=2)),\n",
       "                ('nb', MultinomialNB(alpha=1))])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs3_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if i put nb__alpha in the grid search, best_params for it is 0.1 and we get 0.95 train score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use Pipeline for CountVectorizer Naive Bayes to change max features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gs3_1.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gs3_1_model =gs3_1.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gs3_1_model.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gs3_1_model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gs3_1_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use Pipeline for TfidfVectorizer Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe4 = Pipeline([\n",
    "    ('tvec', TfidfVectorizer()),\n",
    "    ('nb', MultinomialNB()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('tvec', TfidfVectorizer()), ('nb', MultinomialNB())],\n",
       " 'verbose': False,\n",
       " 'tvec': TfidfVectorizer(),\n",
       " 'nb': MultinomialNB(),\n",
       " 'tvec__analyzer': 'word',\n",
       " 'tvec__binary': False,\n",
       " 'tvec__decode_error': 'strict',\n",
       " 'tvec__dtype': numpy.float64,\n",
       " 'tvec__encoding': 'utf-8',\n",
       " 'tvec__input': 'content',\n",
       " 'tvec__lowercase': True,\n",
       " 'tvec__max_df': 1.0,\n",
       " 'tvec__max_features': None,\n",
       " 'tvec__min_df': 1,\n",
       " 'tvec__ngram_range': (1, 1),\n",
       " 'tvec__norm': 'l2',\n",
       " 'tvec__preprocessor': None,\n",
       " 'tvec__smooth_idf': True,\n",
       " 'tvec__stop_words': None,\n",
       " 'tvec__strip_accents': None,\n",
       " 'tvec__sublinear_tf': False,\n",
       " 'tvec__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'tvec__tokenizer': None,\n",
       " 'tvec__use_idf': True,\n",
       " 'tvec__vocabulary': None,\n",
       " 'nb__alpha': 1.0,\n",
       " 'nb__class_prior': None,\n",
       " 'nb__fit_prior': True}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe4.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_params4 = {\n",
    "    'tvec__max_features': [1000, 2000, 3000],\n",
    "    'tvec__min_df': [2,3,4],\n",
    "    'tvec__max_df': [0.9, 0.95],\n",
    "    'tvec__ngram_range': [(1,1), (1,2)],\n",
    "    'nb__alpha': [1, 10, 100]\n",
    "    #'nb__alpha': [0.1, 1, 10, 100]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs4 = GridSearchCV(pipe4, # what object are we optimizing?\n",
    "                  param_grid= pipe_params4, # what parameters values are we searching?\n",
    "                  cv=5) # 5-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('tvec', TfidfVectorizer()),\n",
       "                                       ('nb', MultinomialNB())]),\n",
       "             param_grid={'nb__alpha': [1, 10, 100], 'tvec__max_df': [0.9, 0.95],\n",
       "                         'tvec__max_features': [1000, 2000, 3000],\n",
       "                         'tvec__min_df': [2, 3, 4],\n",
       "                         'tvec__ngram_range': [(1, 1), (1, 2)]})"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs4.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9019555882713778"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs4.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs4_model =gs4.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9502801120448179"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs4_model.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9119496855345912"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs4_model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tvec',\n",
       "                 TfidfVectorizer(max_df=0.9, max_features=2000, min_df=3,\n",
       "                                 ngram_range=(1, 2))),\n",
       "                ('nb', MultinomialNB(alpha=1))])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs4_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe5 = Pipeline([\n",
    "    ('tvec', TfidfVectorizer()),\n",
    "    ('rf', RandomForestClassifier()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('tvec', TfidfVectorizer()), ('rf', RandomForestClassifier())],\n",
       " 'verbose': False,\n",
       " 'tvec': TfidfVectorizer(),\n",
       " 'rf': RandomForestClassifier(),\n",
       " 'tvec__analyzer': 'word',\n",
       " 'tvec__binary': False,\n",
       " 'tvec__decode_error': 'strict',\n",
       " 'tvec__dtype': numpy.float64,\n",
       " 'tvec__encoding': 'utf-8',\n",
       " 'tvec__input': 'content',\n",
       " 'tvec__lowercase': True,\n",
       " 'tvec__max_df': 1.0,\n",
       " 'tvec__max_features': None,\n",
       " 'tvec__min_df': 1,\n",
       " 'tvec__ngram_range': (1, 1),\n",
       " 'tvec__norm': 'l2',\n",
       " 'tvec__preprocessor': None,\n",
       " 'tvec__smooth_idf': True,\n",
       " 'tvec__stop_words': None,\n",
       " 'tvec__strip_accents': None,\n",
       " 'tvec__sublinear_tf': False,\n",
       " 'tvec__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'tvec__tokenizer': None,\n",
       " 'tvec__use_idf': True,\n",
       " 'tvec__vocabulary': None,\n",
       " 'rf__bootstrap': True,\n",
       " 'rf__ccp_alpha': 0.0,\n",
       " 'rf__class_weight': None,\n",
       " 'rf__criterion': 'gini',\n",
       " 'rf__max_depth': None,\n",
       " 'rf__max_features': 'auto',\n",
       " 'rf__max_leaf_nodes': None,\n",
       " 'rf__max_samples': None,\n",
       " 'rf__min_impurity_decrease': 0.0,\n",
       " 'rf__min_impurity_split': None,\n",
       " 'rf__min_samples_leaf': 1,\n",
       " 'rf__min_samples_split': 2,\n",
       " 'rf__min_weight_fraction_leaf': 0.0,\n",
       " 'rf__n_estimators': 100,\n",
       " 'rf__n_jobs': None,\n",
       " 'rf__oob_score': False,\n",
       " 'rf__random_state': None,\n",
       " 'rf__verbose': 0,\n",
       " 'rf__warm_start': False}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe5.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_params5 = {\n",
    "    'tvec__max_features': [1000, 2000, 3000],\n",
    "    'tvec__min_df': [2,3,4],\n",
    "    'tvec__ngram_range': [(1,1), (1,2)],\n",
    "    'rf__n_estimators': [100,200,300],\n",
    "    'rf__max_depth': [None,2,4,6,8],\n",
    "    'rf__max_features': ['sqrt', 'log2']     \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs5 = GridSearchCV(pipe5, param_grid=pipe_params5, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gs5.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gs5.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gs5.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare and choose best model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Name | Vectorizer | Train Score|Test Score|Train/Test Score gap\n",
    "-|-|-|-|-\n",
    "Logistic Regression|CountVectorizer|99.2%|90.8%|8.4%\n",
    "Logistic Regression|TfidfVectorizer|97.5%|92.0%|5.5%\n",
    "Naive Bayes|CountVectorizer|95.7%|93.3%|2.4%\n",
    "Naive Bayes|TfidfVectorizer|95.0%|91.2%|3.8%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the score table, for logistic regression, with both CountVectorizer and TfidfVectorizer, the gap between train score and test score are quite large, i.e. 5.5-8.4% with highly overfitting.\n",
    "\n",
    "For Naive Bayes, with both CountVectorizer and TfidfVectorizer, the gap between train score and test data is smaller, i.e. 2.4-3.8%. The model still overfits but not too much comparing to logistic regression.\n",
    "\n",
    "Among these 4 models, I choose Naive Bayes combined with CountVectorizer as best model because this model does not overfit a lot. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion Table with best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predict tea</th>\n",
       "      <th>predict coffee</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>actual tea</th>\n",
       "      <td>230</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>actual coffee</th>\n",
       "      <td>17</td>\n",
       "      <td>215</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               predict tea  predict coffee\n",
       "actual tea             230              15\n",
       "actual coffee           17             215"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate a confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_preds = gs3.predict(X_test)\n",
    "\n",
    "pd.DataFrame(confusion_matrix(y_test, y_preds),\n",
    "            columns=['predict tea', 'predict coffee'],\n",
    "            index=['actual tea', 'actual coffee'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9329\n",
      "Misclassification rate: 0.0671\n",
      "Precision: 0.9348\n",
      "Recall: 0.9267\n",
      "Specificity: 0.9388\n"
     ]
    }
   ],
   "source": [
    "# Examine some classification metrics \n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_preds).ravel()\n",
    "print('Accuracy: {}'.format(round((tp+tn)/(tp+fp+tn+fn),4)))\n",
    "print('Misclassification rate: {}'.format(round((fp+fn)/(tp+fp+tn+fn),4)))\n",
    "print('Precision: {}'.format(round(tp/(tp+fp),4)))\n",
    "print('Recall: {}'.format(round(tp/(tp+fn),4)))\n",
    "print('Specificity: {}'.format(round(tn/(tn+fp),4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_prob = gs3.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_combine</th>\n",
       "      <th>preds_prob</th>\n",
       "      <th>preds</th>\n",
       "      <th>true_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1697</th>\n",
       "      <td>sorry place ask search internet really find an...</td>\n",
       "      <td>6.917611e-20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1118</th>\n",
       "      <td>recent haul arrive</td>\n",
       "      <td>1.882665e-02</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1907</th>\n",
       "      <td>nice relaxing day dan cong oolong secret spot</td>\n",
       "      <td>4.142709e-04</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1276</th>\n",
       "      <td>nerdy stir heat plate matcha setup work amazing</td>\n",
       "      <td>2.625289e-02</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>hey sorry address wonder bit hope lurker may f...</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800</th>\n",
       "      <td>miroco milk heater frother bit disspointe capa...</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>niche killer jam hoffman</td>\n",
       "      <td>9.924121e-01</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1761</th>\n",
       "      <td>hey everyone boyfriend really also travel want...</td>\n",
       "      <td>7.192477e-04</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>769</th>\n",
       "      <td>light roasted ethiopian yirgacheffe bean smell...</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>work aeropress week think ready allot limit va...</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>477 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          clean_combine    preds_prob  preds  \\\n",
       "1697  sorry place ask search internet really find an...  6.917611e-20      0   \n",
       "1118                                 recent haul arrive  1.882665e-02      0   \n",
       "1907      nice relaxing day dan cong oolong secret spot  4.142709e-04      0   \n",
       "1276    nerdy stir heat plate matcha setup work amazing  2.625289e-02      0   \n",
       "136   hey sorry address wonder bit hope lurker may f...  1.000000e+00      1   \n",
       "...                                                 ...           ...    ...   \n",
       "800   miroco milk heater frother bit disspointe capa...  1.000000e+00      1   \n",
       "133                            niche killer jam hoffman  9.924121e-01      1   \n",
       "1761  hey everyone boyfriend really also travel want...  7.192477e-04      0   \n",
       "769   light roasted ethiopian yirgacheffe bean smell...  1.000000e+00      1   \n",
       "14    work aeropress week think ready allot limit va...  1.000000e+00      1   \n",
       "\n",
       "      true_y  \n",
       "1697       0  \n",
       "1118       0  \n",
       "1907       0  \n",
       "1276       0  \n",
       "136        1  \n",
       "...      ...  \n",
       "800        1  \n",
       "133        1  \n",
       "1761       0  \n",
       "769        1  \n",
       "14         1  \n",
       "\n",
       "[477 rows x 4 columns]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds= pd.DataFrame({\n",
    "    'clean_combine':X_test,\n",
    "    'preds_prob':[preds_prob[i][1] for i in range(len(preds_prob))],\n",
    "    'preds': y_preds,\n",
    "    'true_y':y_test \n",
    "})\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds['diff'] = preds['preds'] - preds['true_y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "buy haru bancha yuuki cha recommend vender gram dry minute still taste light way taste strong gram always taste light way haru bancha strong\n",
      "--------------------\n",
      "see cabinet low crate really milk crate\n",
      "--------------------\n",
      "year journey finally space money table chair issue drag outside want gentle breeze\n",
      "--------------------\n",
      "shoot picture cheap canon cheap haha\n",
      "--------------------\n",
      "startet collect patina thought patina\n",
      "--------------------\n",
      "decide give space\n",
      "--------------------\n",
      "bridal shower party success\n",
      "--------------------\n",
      "aficionado little pot filter build inside ease boiling add extract many gram would say day extract indian ground look loose gifted many day safe extract grind\n",
      "--------------------\n",
      "original gift idea fail great backup\n",
      "--------------------\n",
      "book well day\n",
      "--------------------\n",
      "quick really find answer online head eastern european western asia bit wanting bring back kilo wonder anyone restriction sort family plenty suitcase space issue turkish airline allow check bag per person travel\n",
      "--------------------\n",
      "home old plantation europe\n",
      "--------------------\n",
      "everyone new recently forte african solstice fall love instantly rather expensive anyone recommend dupe look forte african solstice dupe\n",
      "--------------------\n",
      "recently move country place extremely hard notice taste flat bland suddenly wonder method choice prevent thinking something along line britta filter hear suggestion kind filter hard\n",
      "--------------------\n",
      "anyone porous please help\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "# predict is coffee post, but actual is tea post\n",
    "\n",
    "for i in preds.loc[preds['diff'] == 1].clean_combine:\n",
    "    print(i)\n",
    "    print('-'*20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Inside these mis-prediction, there are some words, like 'taste', 'would', etc are among the top words in both coffee and tea CountVectorizer tokens. We can consider to remove the common words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_combine</th>\n",
       "      <th>preds_prob</th>\n",
       "      <th>preds</th>\n",
       "      <th>true_y</th>\n",
       "      <th>diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>hello import family farm salvador united state...</td>\n",
       "      <td>7.780633e-06</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421</th>\n",
       "      <td>hello guy look purchase mineral mixture simila...</td>\n",
       "      <td>1.315962e-01</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>last year seem clear market high quality seem ...</td>\n",
       "      <td>3.181742e-02</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>615</th>\n",
       "      <td>hello tour latin america currently guatemala l...</td>\n",
       "      <td>3.759533e-03</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>anyone townsend english heritage taste history...</td>\n",
       "      <td>3.385136e-04</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>amazon silver wilfa svart worth pick price wil...</td>\n",
       "      <td>1.294736e-01</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>578</th>\n",
       "      <td>guy great take turkey baster force intake forc...</td>\n",
       "      <td>1.785990e-01</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>big bottle infuse mint leave lead idea anyone ...</td>\n",
       "      <td>2.699787e-06</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>howdy love taste however totally caffeine into...</td>\n",
       "      <td>2.256067e-01</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378</th>\n",
       "      <td>gon take guess sub put working research projec...</td>\n",
       "      <td>1.388888e-03</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>pour whistle kettle leave behind person apartm...</td>\n",
       "      <td>8.293394e-02</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>cross country road trip can not truck stop con...</td>\n",
       "      <td>7.910579e-02</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>641</th>\n",
       "      <td>wonder anyone shed light seem two identical ce...</td>\n",
       "      <td>5.417262e-04</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>647</th>\n",
       "      <td>really seem talk probably miss lot great ignor...</td>\n",
       "      <td>4.927495e-02</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>922</th>\n",
       "      <td>say craft beer people probably assume somethin...</td>\n",
       "      <td>3.235573e-01</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>little bit context give background two plant t...</td>\n",
       "      <td>1.313090e-20</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>784</th>\n",
       "      <td>blind sample pack angel see lot people talk an...</td>\n",
       "      <td>6.234144e-03</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         clean_combine    preds_prob  preds  \\\n",
       "566  hello import family farm salvador united state...  7.780633e-06      0   \n",
       "421  hello guy look purchase mineral mixture simila...  1.315962e-01      0   \n",
       "181  last year seem clear market high quality seem ...  3.181742e-02      0   \n",
       "615  hello tour latin america currently guatemala l...  3.759533e-03      0   \n",
       "345  anyone townsend english heritage taste history...  3.385136e-04      0   \n",
       "362  amazon silver wilfa svart worth pick price wil...  1.294736e-01      0   \n",
       "578  guy great take turkey baster force intake forc...  1.785990e-01      0   \n",
       "178  big bottle infuse mint leave lead idea anyone ...  2.699787e-06      0   \n",
       "117  howdy love taste however totally caffeine into...  2.256067e-01      0   \n",
       "378  gon take guess sub put working research projec...  1.388888e-03      0   \n",
       "197  pour whistle kettle leave behind person apartm...  8.293394e-02      0   \n",
       "131  cross country road trip can not truck stop con...  7.910579e-02      0   \n",
       "641  wonder anyone shed light seem two identical ce...  5.417262e-04      0   \n",
       "647  really seem talk probably miss lot great ignor...  4.927495e-02      0   \n",
       "922  say craft beer people probably assume somethin...  3.235573e-01      0   \n",
       "477  little bit context give background two plant t...  1.313090e-20      0   \n",
       "784  blind sample pack angel see lot people talk an...  6.234144e-03      0   \n",
       "\n",
       "     true_y  diff  \n",
       "566       1    -1  \n",
       "421       1    -1  \n",
       "181       1    -1  \n",
       "615       1    -1  \n",
       "345       1    -1  \n",
       "362       1    -1  \n",
       "578       1    -1  \n",
       "178       1    -1  \n",
       "117       1    -1  \n",
       "378       1    -1  \n",
       "197       1    -1  \n",
       "131       1    -1  \n",
       "641       1    -1  \n",
       "647       1    -1  \n",
       "922       1    -1  \n",
       "477       1    -1  \n",
       "784       1    -1  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.loc[preds['diff'] == -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello import family farm salvador united state start import already roast package sell online future import green roasted package anyone need type fda certification anything import already roast package united state would love help seem find anything fda website process need also anyone recommendation freight forwarder someone help custom process since would first time import help advice would appreciate thank import salvador usa\n",
      "--------------------\n",
      "hello guy look purchase mineral mixture similar offer perfect third wave live europe want pay euro ship european company supply similar product thank mineral mix\n",
      "--------------------\n",
      "last year seem clear market high quality seem people want high quality small viable market high scoring quality price regular quality market top quality third wave\n",
      "--------------------\n",
      "hello tour latin america currently guatemala look comprehensive map encyclopedia different type grow central south america visit anyone recommend book website find list map format look depth guide available want miss anything thank help look map variety latin america\n",
      "--------------------\n",
      "anyone townsend english heritage taste history oat milk nut milk modern era nut old way prepare say nut milk forme cury cawdel almaund mylk involve blanch almond likely soak mix wine add spice case ground dry ginger saffron boiling serve cawdel almaund mylk iiii vii take almaunde blaunche drawe hem wyne erto powdour gyngur sugur colour safroun boile serue forth excerpt forme cury samuel pegge material may protect copyright nut milk wonder happen let mix bit medieval recipe meet modern era\n",
      "--------------------\n",
      "amazon silver wilfa svart worth pick price wilfa svart cheap\n",
      "--------------------\n",
      "guy great take turkey baster force intake force air behind magic sound pump braun maker\n",
      "--------------------\n",
      "big bottle infuse mint leave lead idea anyone ever infusion taste would happen mint infuse\n",
      "--------------------\n",
      "howdy love taste however totally caffeine intolerant relegate decaf anybody wisdom impart brewing method decaf blend flavorful decaf blend recommend thank decaffeinate tip recommendation\n",
      "--------------------\n",
      "gon take guess sub put working research project production peru wonder common processing method wet dry find many different answer consider put appear thank advance eta thank lovely redditor answer appear almost entirely wet process future reference wet dry processing peru\n",
      "--------------------\n",
      "pour whistle kettle leave behind person apartment move airplane love free stuff method right temperature take stove couple minute whistle listen boil right away also green want low temperature thinking bonavita digital kettle maybe stove top kettle thermometer would also work camp stove post apocalyptic time digital kettle significantly improve brewing experience stove top kettle thermometer wish pay digital kettle digital gooseneck kettle worth\n",
      "--------------------\n",
      "cross country road trip can not truck stop convenient way pour worry ceramic break tia travel pour\n",
      "--------------------\n",
      "wonder anyone shed light seem two identical cerro azul geshas farm producer process even taste note bird rock bird rock sell even price difference bird rock aaa something matter premium brand bird rock compare able charge anyone insight industry pricing\n",
      "--------------------\n",
      "really seem talk probably miss lot great ignorance favorite ever brazil fazenda rio verde ipanema grand cru apricot hasbean last year beginner grinder moccamaster without skill ever truly love taste pure fruit juice incredible well expensive rare gesha since find today natural buy last year develop preconceived notion naturally process cut way add sugar goal stop add look forward super sweet fruity taste enjoy natural amazing fruit flavor buy enjoy want fruity fermenty fermenty funky taste kill think black white sweet bloom may start great natural roaster last year decide order recommend bag send italy enjoy hate natural\n",
      "--------------------\n",
      "say craft beer people probably assume something well bud light say without people think lot starbuck edit walk around tell stranger expensive new job boss want write short bio send employee want say without sound talk morning person tell people\n",
      "--------------------\n",
      "little bit context give background two plant tip advice help recover flourish relevant info zone tropical level humidity despite tree flower produce minimal amount bean currently bean total past two year production increase picture two tree story time keep short year ago move apartment complex prior resident leave small dry pot die plant outside back patio decoration figure revive keep without type plant time plant approximately tall sparse branch die leave completely cover aphid bug move early fall temperature manage shock plant nearly leave shock fall branch exclude top growth three plant succumb weather never recover manage save two next year struggle experience spring growth lose decent amount leave even full branch due winter dry climate inside apartment overall lack knowledge plant first place still specie day grow seemingly happy recall correctly tree produce fruit point prompt upsize repot last spring large gal pot past year completely rocket growth today foot tall maybe flower two february past month hundred bud dozen ripen ready flower within week yay throughout entire process well humidify apartment prevent leaf loss difficult especially winter past week grab humidifier see possibly place humid enough also spray leave mister time day past week despite still start brown leave dry annoyance otherwise help promote branch growth seem fine grow upwards base tree really barren funky look due harsh revival anyway main pinch top growth way help promote branch growth towards base tree basically stalk repopulate trunk possible also advice prevent brown tip completely dry leave whatever keep plant humid seem keep leave completely green unfortunate result location either way hope bloom produce exponentially fruit year would love advice increase branch volume overall width shape tree thank advance appreciate pruning care advice rescue tree help appreciate\n",
      "--------------------\n",
      "blind sample pack angel see lot people talk angel anymore wonder anyone else subscription past tip advice angel flight\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "# predict is tea post, but actual is coffee post\n",
    "\n",
    "for i in preds.loc[preds['diff'] == -1].clean_combine:\n",
    "    print(i)\n",
    "    print('-'*20)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Most of these posts have the word 'anyone' in them, which could be a significant term that misclassified them as tea posts. 'anyone' is in the top20 word list for tea post CountVectorize tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": "4",
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
