{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Project 3: Web APIs & Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "\n",
    "Using Reddit's API, you'll collect posts from two subreddits of your choosing.\n",
    "You'll then use NLP to train a classifier on which subreddit a given post came from. This is a binary classification problem.\n",
    "Create and compare two models. One of these must be a Bayes classifier, however the other can be a classifier of your choosing: logistic regression, KNN, SVM, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Executive Summary\n",
    "The Ames housing dataset includes 80 features of nominal, discrete, ordinal and continuous variables for individual residential properties sold in Ames, IA from 2006 to 2010.\n",
    "\n",
    "During the first step workflow which is data cleaning, missing values were detected and fixed, outlier points are investigated and eliminitated from the dataset. Once data cleaning is done, Exploratory Data Analysis (EDA) is conducted for each feature. Ordinal features are converted to discrete values. For categorical data (nominal), bar plots were created to visualize the mean Sale Price across categories. During EDA for categorical features, special attention was paid to any patterns or clusters in Sale Prices that emerged. I also expore continuous/discrete features with similar groups, like sold/built year, bathroom No and floor SF to create more meaningful new features. \n",
    "\n",
    "The linear relationship between Sale Price and all numeric features were examined using heatmap and correlation coefficients. Features with high correlation rate (>=0.4) were filtered out for visualization check. \n",
    "Features with continous data were plotted with scatter plot while features with discrete data were plotted with boxplot. \n",
    "Heatmap for all filtered-out features were also plotted to check colinear between features. If there was high colinearity between features, the feature with less correlation rate with Sale Price was removed from filtered features list. \n",
    "\n",
    "After filtered features list were comfirmed, train dataset was divided into training set (70% of data) and holdout set (30% of data) to prepare for modeling. In this project, 4 models (linear regression/ridge regression/lasso regression/elsticNET regression) were used. \n",
    "\n",
    "In the first verification, filtered features list with 27 features from EDA were used for these 4 models. Among these models, elasticNET had best MSE score. However, I observed that with higher Sale Price, the best fit of line did not fit well. In the high sale price side, the line tended to be curly. We decided to add square value (power 2) for some features which had high correction with saleprice to observe whether could improve MSE score and best fit of line in second verification. \n",
    "\n",
    "In the second verification, 6 square values (power 2) were added. The result showed much better MSE and slight improvement of best fit line for higher Sale Price. So I decided to try higher power into the features, i.e. add power 3 to verify whether higher power value could further improve the models in the third verification. \n",
    "\n",
    "In the third verification, 6 power 3 values were added. MSE scores were improved slightly. By checking lasso regression coeffient, some features' coeffients were zero. This may be due to high colinearity among the feature and its power 2 / power 3. In the fourth verification, features with zero coeffient were dropped. \n",
    "\n",
    "\n",
    "In the fourth verification, the MSE score almost had no improvement. However, we can observe the samll gap between train data MSE score and holdout data MSE score. R2 score for train dataset and holdout data set were 91% and 90% respectively. We can say that this model could fit train and holdout data well. \n",
    "\n",
    "After we identify our best model, residuals plot was evaluated and Sale Prices in test dataset were predicted. Interpretations and recommendations were made based off of the best-performing model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plan to choose r/Coffee & r/Tea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import requests\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grab_post(topic):\n",
    "    \n",
    "    posts = []\n",
    "    after = None\n",
    "    url = 'https://www.reddit.com/r/'+topic+'.json'\n",
    "\n",
    "    for a in range(40):\n",
    "        if after == None:\n",
    "            current_url = url\n",
    "        else:\n",
    "            current_url = url + '?after=' + after\n",
    "        print(current_url)\n",
    "        res = requests.get(current_url, headers={'User-agent': 'YL Agent'})\n",
    "\n",
    "        if res.status_code != 200:\n",
    "            print('Status error', res.status_code)\n",
    "            break\n",
    "\n",
    "        current_dict = res.json()\n",
    "        current_posts = [p['data'] for p in current_dict['data']['children']]\n",
    "        posts.extend(current_posts)\n",
    "        after = current_dict['data']['after']\n",
    "\n",
    "        # generate a random sleep duration to look more 'natural'\n",
    "        sleep_duration = random.randint(2,6)\n",
    "        print(sleep_duration)\n",
    "        time.sleep(sleep_duration)\n",
    "      \n",
    "    pd.DataFrame(posts).to_csv(\"../data/\"+topic+\".csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.reddit.com/r/Tea.json\n",
      "3\n",
      "https://www.reddit.com/r/Tea.json?after=t3_o51yit\n",
      "3\n",
      "https://www.reddit.com/r/Tea.json?after=t3_o4kuzn\n",
      "4\n",
      "https://www.reddit.com/r/Tea.json?after=t3_o3j7gr\n",
      "6\n",
      "https://www.reddit.com/r/Tea.json?after=t3_o32arr\n",
      "4\n",
      "https://www.reddit.com/r/Tea.json?after=t3_o1py6g\n",
      "3\n",
      "https://www.reddit.com/r/Tea.json?after=t3_o1m7vj\n",
      "5\n",
      "https://www.reddit.com/r/Tea.json?after=t3_o0q8q7\n",
      "6\n",
      "https://www.reddit.com/r/Tea.json?after=t3_nzqhi2\n",
      "5\n",
      "https://www.reddit.com/r/Tea.json?after=t3_nz02je\n",
      "4\n",
      "https://www.reddit.com/r/Tea.json?after=t3_nylgjr\n",
      "3\n",
      "https://www.reddit.com/r/Tea.json?after=t3_nyc8cd\n",
      "6\n",
      "https://www.reddit.com/r/Tea.json?after=t3_nxccoz\n",
      "4\n",
      "https://www.reddit.com/r/Tea.json?after=t3_nwg554\n",
      "3\n",
      "https://www.reddit.com/r/Tea.json?after=t3_nvx641\n",
      "2\n",
      "https://www.reddit.com/r/Tea.json?after=t3_nv1eb3\n",
      "6\n",
      "https://www.reddit.com/r/Tea.json?after=t3_nu7w5x\n",
      "6\n",
      "https://www.reddit.com/r/Tea.json?after=t3_ntfstp\n",
      "4\n",
      "https://www.reddit.com/r/Tea.json?after=t3_nsaop6\n",
      "5\n",
      "https://www.reddit.com/r/Tea.json?after=t3_nroo3l\n",
      "6\n",
      "https://www.reddit.com/r/Tea.json?after=t3_nr6699\n",
      "2\n",
      "https://www.reddit.com/r/Tea.json?after=t3_npjef5\n",
      "2\n",
      "https://www.reddit.com/r/Tea.json?after=t3_npqy34\n",
      "3\n",
      "https://www.reddit.com/r/Tea.json?after=t3_np196m\n",
      "4\n",
      "https://www.reddit.com/r/Tea.json?after=t3_no6imj\n",
      "4\n",
      "https://www.reddit.com/r/Tea.json?after=t3_nn7pbl\n",
      "6\n",
      "https://www.reddit.com/r/Tea.json?after=t3_nn8l4i\n",
      "3\n",
      "https://www.reddit.com/r/Tea.json?after=t3_nmii0k\n",
      "6\n",
      "https://www.reddit.com/r/Tea.json?after=t3_nm4a4m\n",
      "3\n",
      "https://www.reddit.com/r/Tea.json?after=t3_nltnwr\n",
      "3\n",
      "https://www.reddit.com/r/Tea.json?after=t3_nkyq3v\n",
      "6\n",
      "https://www.reddit.com/r/Tea.json?after=t3_nkjewi\n",
      "6\n",
      "https://www.reddit.com/r/Tea.json?after=t3_njf0ef\n",
      "4\n",
      "https://www.reddit.com/r/Tea.json?after=t3_niydgk\n",
      "2\n",
      "https://www.reddit.com/r/Tea.json?after=t3_nhzv71\n",
      "6\n",
      "https://www.reddit.com/r/Tea.json?after=t3_nhre60\n",
      "5\n",
      "https://www.reddit.com/r/Tea.json?after=t3_nhhiwg\n",
      "2\n",
      "https://www.reddit.com/r/Tea.json?after=t3_ng5xmw\n",
      "3\n",
      "https://www.reddit.com/r/Tea.json?after=t3_nfjdqd\n",
      "3\n",
      "https://www.reddit.com/r/Tea.json?after=t3_nf7p5r\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "grab_post('Tea')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.reddit.com/r/Coffee.json\n",
      "4\n",
      "https://www.reddit.com/r/Coffee.json?after=t3_o3q04o\n",
      "2\n",
      "https://www.reddit.com/r/Coffee.json?after=t3_o1wpx6\n",
      "5\n",
      "https://www.reddit.com/r/Coffee.json?after=t3_o0ap08\n",
      "3\n",
      "https://www.reddit.com/r/Coffee.json?after=t3_nzpcba\n",
      "5\n",
      "https://www.reddit.com/r/Coffee.json?after=t3_nxviug\n",
      "5\n",
      "https://www.reddit.com/r/Coffee.json?after=t3_nv2tug\n",
      "2\n",
      "https://www.reddit.com/r/Coffee.json?after=t3_ntwm1b\n",
      "4\n",
      "https://www.reddit.com/r/Coffee.json?after=t3_ns7sxd\n",
      "5\n",
      "https://www.reddit.com/r/Coffee.json?after=t3_nqi9ch\n",
      "5\n",
      "https://www.reddit.com/r/Coffee.json?after=t3_npjq4t\n",
      "6\n",
      "https://www.reddit.com/r/Coffee.json?after=t3_no7zg6\n",
      "2\n",
      "https://www.reddit.com/r/Coffee.json?after=t3_nmzzaq\n",
      "4\n",
      "https://www.reddit.com/r/Coffee.json?after=t3_nl7fnv\n",
      "5\n",
      "https://www.reddit.com/r/Coffee.json?after=t3_nk8op0\n",
      "4\n",
      "https://www.reddit.com/r/Coffee.json?after=t3_nj4kad\n",
      "4\n",
      "https://www.reddit.com/r/Coffee.json?after=t3_ngwxmb\n",
      "2\n",
      "https://www.reddit.com/r/Coffee.json?after=t3_ngb5js\n",
      "4\n",
      "https://www.reddit.com/r/Coffee.json?after=t3_ndy9m5\n",
      "6\n",
      "https://www.reddit.com/r/Coffee.json?after=t3_ncz6ov\n",
      "2\n",
      "https://www.reddit.com/r/Coffee.json?after=t3_nakdy7\n",
      "5\n",
      "https://www.reddit.com/r/Coffee.json?after=t3_n8giiq\n",
      "4\n",
      "https://www.reddit.com/r/Coffee.json?after=t3_n6uyvh\n",
      "3\n",
      "https://www.reddit.com/r/Coffee.json?after=t3_n5f5c2\n",
      "6\n",
      "https://www.reddit.com/r/Coffee.json?after=t3_n4j58t\n",
      "6\n",
      "https://www.reddit.com/r/Coffee.json?after=t3_n3a700\n",
      "3\n",
      "https://www.reddit.com/r/Coffee.json?after=t3_n27v5j\n",
      "4\n",
      "https://www.reddit.com/r/Coffee.json?after=t3_n0c2yj\n",
      "6\n",
      "https://www.reddit.com/r/Coffee.json?after=t3_mzcf80\n",
      "4\n",
      "https://www.reddit.com/r/Coffee.json?after=t3_mympf9\n",
      "5\n",
      "https://www.reddit.com/r/Coffee.json?after=t3_mxhk9o\n",
      "4\n",
      "https://www.reddit.com/r/Coffee.json?after=t3_mwruuc\n",
      "4\n",
      "https://www.reddit.com/r/Coffee.json?after=t3_mv68gv\n",
      "4\n",
      "https://www.reddit.com/r/Coffee.json?after=t3_mu6pb0\n",
      "5\n",
      "https://www.reddit.com/r/Coffee.json?after=t3_mtwxaf\n",
      "3\n",
      "https://www.reddit.com/r/Coffee.json?after=t3_mswxa7\n",
      "3\n",
      "https://www.reddit.com/r/Coffee.json?after=t3_mroxfp\n",
      "6\n",
      "https://www.reddit.com/r/Coffee.json\n",
      "3\n",
      "https://www.reddit.com/r/Coffee.json?after=t3_o3q04o\n",
      "3\n",
      "https://www.reddit.com/r/Coffee.json?after=t3_o1wpx6\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "grab_post('Coffee')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": "1",
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "284.433px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
