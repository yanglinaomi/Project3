{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Project 3: Web APIs & Classification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "\n",
    "KiddyToy sells wonderful and liable children toys. From last year, KiddyToy made decision to move their business online and also provided online customer feedback service. This increases company's revenue significantly. Company wants to utilize the customer feedback to improve the toy manufacturing and business decision-making. Customer service team will categorize customer's feedback as \"customer complain\" or \"customer support\".\n",
    "\n",
    "However, KiddyToy faces the problem that receive drastic amount of customer feedback each day. The current system only allows employees to categorize customer feedback manually which is time-consuming. As the company's data science team member, we initialize a project to build up a text classifier to automate this process. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Executive Summary\n",
    "\n",
    "\n",
    "It's estimated that around 80% of all information is unstructured, with text being one of the most common types of unstructured data. Because of the messy nature of text, analyzing, understanding, organizing, and sorting through text data is hard and time-consuming. KiddyToy's customer service team faces the same issue. \n",
    "\n",
    "In this project, we develop a text classifier to help KiddyToy's customer service team. We select two subreddits, /r/Coffee and /r/Tea from reddit website. The choice of these two subreddits is motivated by their text-heavy posts. The goal of this project is to classify which subreddit a given post came from. We create and compare two models: logistic regression and naive Bayes classifier and choose the best model. Our best model can perform well with a test accuracy score of 92.05%. \n",
    "\n",
    "By using this text classifier, customer service team can automatically structure texts from customer feedback in a fast and cost-effective way. This allows companies to save time analyzing text data, automate business processes, and improve customer satisfaction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we choose subreddits Coffee and Tea which share enough similarities to provide a good challenge, but are also differentiated enough that it should be possible to train a machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import library\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import requests\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grab_post(topic):\n",
    "    \n",
    "    posts = []\n",
    "    after = None\n",
    "    url = 'https://www.reddit.com/r/'+topic+'.json'\n",
    "\n",
    "    for a in range(40):\n",
    "        if after == None:\n",
    "            current_url = url\n",
    "        else:\n",
    "            current_url = url + '?after=' + after\n",
    "        print(current_url)\n",
    "        res = requests.get(current_url, headers={'User-agent': 'YL Agent'})\n",
    "\n",
    "        if res.status_code != 200:\n",
    "            print('Status error', res.status_code)\n",
    "            break\n",
    "\n",
    "        current_dict = res.json()\n",
    "        current_posts = [p['data'] for p in current_dict['data']['children']]\n",
    "        posts.extend(current_posts)\n",
    "        after = current_dict['data']['after']\n",
    "\n",
    "        # generate a random sleep duration to look more 'natural'\n",
    "        sleep_duration = random.randint(2,6)\n",
    "        print(sleep_duration)\n",
    "        time.sleep(sleep_duration)\n",
    "        \n",
    "    # save raw data to csv  \n",
    "    pd.DataFrame(posts).to_csv(\"../data/\"+topic+\".csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.reddit.com/r/Tea.json\n",
      "3\n",
      "https://www.reddit.com/r/Tea.json?after=t3_o51yit\n",
      "3\n",
      "https://www.reddit.com/r/Tea.json?after=t3_o4kuzn\n",
      "4\n",
      "https://www.reddit.com/r/Tea.json?after=t3_o3j7gr\n",
      "6\n",
      "https://www.reddit.com/r/Tea.json?after=t3_o32arr\n",
      "4\n",
      "https://www.reddit.com/r/Tea.json?after=t3_o1py6g\n",
      "3\n",
      "https://www.reddit.com/r/Tea.json?after=t3_o1m7vj\n",
      "5\n",
      "https://www.reddit.com/r/Tea.json?after=t3_o0q8q7\n",
      "6\n",
      "https://www.reddit.com/r/Tea.json?after=t3_nzqhi2\n",
      "5\n",
      "https://www.reddit.com/r/Tea.json?after=t3_nz02je\n",
      "4\n",
      "https://www.reddit.com/r/Tea.json?after=t3_nylgjr\n",
      "3\n",
      "https://www.reddit.com/r/Tea.json?after=t3_nyc8cd\n",
      "6\n",
      "https://www.reddit.com/r/Tea.json?after=t3_nxccoz\n",
      "4\n",
      "https://www.reddit.com/r/Tea.json?after=t3_nwg554\n",
      "3\n",
      "https://www.reddit.com/r/Tea.json?after=t3_nvx641\n",
      "2\n",
      "https://www.reddit.com/r/Tea.json?after=t3_nv1eb3\n",
      "6\n",
      "https://www.reddit.com/r/Tea.json?after=t3_nu7w5x\n",
      "6\n",
      "https://www.reddit.com/r/Tea.json?after=t3_ntfstp\n",
      "4\n",
      "https://www.reddit.com/r/Tea.json?after=t3_nsaop6\n",
      "5\n",
      "https://www.reddit.com/r/Tea.json?after=t3_nroo3l\n",
      "6\n",
      "https://www.reddit.com/r/Tea.json?after=t3_nr6699\n",
      "2\n",
      "https://www.reddit.com/r/Tea.json?after=t3_npjef5\n",
      "2\n",
      "https://www.reddit.com/r/Tea.json?after=t3_npqy34\n",
      "3\n",
      "https://www.reddit.com/r/Tea.json?after=t3_np196m\n",
      "4\n",
      "https://www.reddit.com/r/Tea.json?after=t3_no6imj\n",
      "4\n",
      "https://www.reddit.com/r/Tea.json?after=t3_nn7pbl\n",
      "6\n",
      "https://www.reddit.com/r/Tea.json?after=t3_nn8l4i\n",
      "3\n",
      "https://www.reddit.com/r/Tea.json?after=t3_nmii0k\n",
      "6\n",
      "https://www.reddit.com/r/Tea.json?after=t3_nm4a4m\n",
      "3\n",
      "https://www.reddit.com/r/Tea.json?after=t3_nltnwr\n",
      "3\n",
      "https://www.reddit.com/r/Tea.json?after=t3_nkyq3v\n",
      "6\n",
      "https://www.reddit.com/r/Tea.json?after=t3_nkjewi\n",
      "6\n",
      "https://www.reddit.com/r/Tea.json?after=t3_njf0ef\n",
      "4\n",
      "https://www.reddit.com/r/Tea.json?after=t3_niydgk\n",
      "2\n",
      "https://www.reddit.com/r/Tea.json?after=t3_nhzv71\n",
      "6\n",
      "https://www.reddit.com/r/Tea.json?after=t3_nhre60\n",
      "5\n",
      "https://www.reddit.com/r/Tea.json?after=t3_nhhiwg\n",
      "2\n",
      "https://www.reddit.com/r/Tea.json?after=t3_ng5xmw\n",
      "3\n",
      "https://www.reddit.com/r/Tea.json?after=t3_nfjdqd\n",
      "3\n",
      "https://www.reddit.com/r/Tea.json?after=t3_nf7p5r\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "grab_post('Tea')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.reddit.com/r/Coffee.json\n",
      "4\n",
      "https://www.reddit.com/r/Coffee.json?after=t3_o3q04o\n",
      "2\n",
      "https://www.reddit.com/r/Coffee.json?after=t3_o1wpx6\n",
      "5\n",
      "https://www.reddit.com/r/Coffee.json?after=t3_o0ap08\n",
      "3\n",
      "https://www.reddit.com/r/Coffee.json?after=t3_nzpcba\n",
      "5\n",
      "https://www.reddit.com/r/Coffee.json?after=t3_nxviug\n",
      "5\n",
      "https://www.reddit.com/r/Coffee.json?after=t3_nv2tug\n",
      "2\n",
      "https://www.reddit.com/r/Coffee.json?after=t3_ntwm1b\n",
      "4\n",
      "https://www.reddit.com/r/Coffee.json?after=t3_ns7sxd\n",
      "5\n",
      "https://www.reddit.com/r/Coffee.json?after=t3_nqi9ch\n",
      "5\n",
      "https://www.reddit.com/r/Coffee.json?after=t3_npjq4t\n",
      "6\n",
      "https://www.reddit.com/r/Coffee.json?after=t3_no7zg6\n",
      "2\n",
      "https://www.reddit.com/r/Coffee.json?after=t3_nmzzaq\n",
      "4\n",
      "https://www.reddit.com/r/Coffee.json?after=t3_nl7fnv\n",
      "5\n",
      "https://www.reddit.com/r/Coffee.json?after=t3_nk8op0\n",
      "4\n",
      "https://www.reddit.com/r/Coffee.json?after=t3_nj4kad\n",
      "4\n",
      "https://www.reddit.com/r/Coffee.json?after=t3_ngwxmb\n",
      "2\n",
      "https://www.reddit.com/r/Coffee.json?after=t3_ngb5js\n",
      "4\n",
      "https://www.reddit.com/r/Coffee.json?after=t3_ndy9m5\n",
      "6\n",
      "https://www.reddit.com/r/Coffee.json?after=t3_ncz6ov\n",
      "2\n",
      "https://www.reddit.com/r/Coffee.json?after=t3_nakdy7\n",
      "5\n",
      "https://www.reddit.com/r/Coffee.json?after=t3_n8giiq\n",
      "4\n",
      "https://www.reddit.com/r/Coffee.json?after=t3_n6uyvh\n",
      "3\n",
      "https://www.reddit.com/r/Coffee.json?after=t3_n5f5c2\n",
      "6\n",
      "https://www.reddit.com/r/Coffee.json?after=t3_n4j58t\n",
      "6\n",
      "https://www.reddit.com/r/Coffee.json?after=t3_n3a700\n",
      "3\n",
      "https://www.reddit.com/r/Coffee.json?after=t3_n27v5j\n",
      "4\n",
      "https://www.reddit.com/r/Coffee.json?after=t3_n0c2yj\n",
      "6\n",
      "https://www.reddit.com/r/Coffee.json?after=t3_mzcf80\n",
      "4\n",
      "https://www.reddit.com/r/Coffee.json?after=t3_mympf9\n",
      "5\n",
      "https://www.reddit.com/r/Coffee.json?after=t3_mxhk9o\n",
      "4\n",
      "https://www.reddit.com/r/Coffee.json?after=t3_mwruuc\n",
      "4\n",
      "https://www.reddit.com/r/Coffee.json?after=t3_mv68gv\n",
      "4\n",
      "https://www.reddit.com/r/Coffee.json?after=t3_mu6pb0\n",
      "5\n",
      "https://www.reddit.com/r/Coffee.json?after=t3_mtwxaf\n",
      "3\n",
      "https://www.reddit.com/r/Coffee.json?after=t3_mswxa7\n",
      "3\n",
      "https://www.reddit.com/r/Coffee.json?after=t3_mroxfp\n",
      "6\n",
      "https://www.reddit.com/r/Coffee.json\n",
      "3\n",
      "https://www.reddit.com/r/Coffee.json?after=t3_o3q04o\n",
      "3\n",
      "https://www.reddit.com/r/Coffee.json?after=t3_o1wpx6\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "grab_post('Coffee')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": "1",
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "284.433px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
