{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/train_combine.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "subreddit        0\n",
       "combine_text     0\n",
       "clean_combine    2\n",
       "is_coffee        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check whether any null for df\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>combine_text</th>\n",
       "      <th>clean_combine</th>\n",
       "      <th>is_coffee</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>786</th>\n",
       "      <td>Coffee</td>\n",
       "      <td>It's here!</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1658</th>\n",
       "      <td>tea</td>\n",
       "      <td>I was looking to order some tea and I stumbled...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     subreddit                                       combine_text  \\\n",
       "786     Coffee                                         It's here!   \n",
       "1658       tea  I was looking to order some tea and I stumbled...   \n",
       "\n",
       "     clean_combine  is_coffee  \n",
       "786            NaN          1  \n",
       "1658           NaN          0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# There are two null for clean_combine. probably clean_combine is empty during preprocessing. \n",
    "# We will drop these two rows.\n",
    "df.loc[df['clean_combine'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define X, y and check baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define X, y \n",
    "X = df['clean_combine']\n",
    "y = df['is_coffee']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.515453\n",
       "1    0.484547\n",
       "Name: is_coffee, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# It is an almost balanced class distribution\n",
    "y.value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train/test/split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train_test_split.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size = 0.25,\n",
    "                                                    random_state = 42,\n",
    "                                                    stratify = y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use CountVectorizer  for Logisctic regression and Naive Bayes (default setting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Logistic Regression model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate Countectorizer\n",
    "cvec = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit_trasform for X_train and transform for X_test\n",
    "train_data_features = cvec.fit_transform(X_train)\n",
    "\n",
    "test_data_features =cvec.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1431, 6168)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# There are 6168 features in our train/test dataset\n",
    "train_data_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9951083158630328"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate logistic regression model.\n",
    "lr = LogisticRegression()\n",
    "\n",
    "# Fit model to training data.\n",
    "lr.fit(train_data_features, y_train)\n",
    "\n",
    "# Evaluate model on training data.\n",
    "lr.score(train_data_features, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9184100418410042"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Accuray score for test data\n",
    "lr.score(test_data_features, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Naive Bayes model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate naive bayes model.\n",
    "nb = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9622641509433962"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit model to training data.\n",
    "nb.fit(train_data_features, y_train)\n",
    "\n",
    "# Evaluate model on training data.\n",
    "nb.score(train_data_features, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9079497907949791"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Accuracy score for test data\n",
    "nb.score(test_data_features, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use TfidfVectorizer for Logisctic regression and Naive Bayes ( default setting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Logistic Regression model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate Countectorizer\n",
    "tvec = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit_trasform for X_train and transform for X_test\n",
    "train_data_features = tvec.fit_transform(X_train)\n",
    "\n",
    "test_data_features =tvec.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9713487071977638"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate logistic regression model.\n",
    "lr = LogisticRegression()\n",
    "\n",
    "# Fit model to training data.\n",
    "lr.fit(train_data_features, y_train)\n",
    "\n",
    "# Evaluate model on training data.\n",
    "lr.score(train_data_features, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9163179916317992"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Accuracy score for test data\n",
    "lr.score(test_data_features, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Naive Bayes model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate Naive Bayes model.\n",
    "nb = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9671558350803634"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fit model to training data.\n",
    "nb.fit(train_data_features, y_train)\n",
    "\n",
    "# Evaluate model on training data.\n",
    "nb.score(train_data_features, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9058577405857741"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Accuracy score for test data\n",
    "nb.score(test_data_features, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Observation (with default setting)**: \n",
    "\n",
    "Model Name | Vectorizer |Train Score|Test Score|Train/Test Score gap\n",
    "-|-|-|-|-\n",
    "Logistic Regression|CountVectorizer|99.5%|91.8%|7.7%\n",
    "Logistic Regression|TfidfVectorizer|97.1%|91.6%|5.5%\n",
    "Naive Bayes|CountVectorizer|96.2%|90.8%|5.4%\n",
    "Naive Bayes|TfidfVectorizer|96.7%|90.6%|6.1%\n",
    "\n",
    "\n",
    "Both Logistic Regression and Naive Bayes models are overfitting. There are total 6168 features in our train dataset. More features cause overfitting for our models, hence good accuracy score for train data. Comparing to Naive Bayes model, Logistic Regression has a better train score but more overfitting. \n",
    "\n",
    "For logistic regression, if applying CountVectorizer, the model overfits more than applying TfidfVectorizer. It seems that high count words by CountVectorizer have strong effect for the model. \n",
    "\n",
    "For Naive Bayes model, there is no much difference by apply CountVectorizer or TfidfVectorizer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In next section, we will use pipeline and GridSearch to build several models to tune the hyperparameters to obtain a best model and also reduce overfit which we observe with default setting in 4.3.1 and 4.3.2.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use Pipeline for CountVectorizer Logisctic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate pipeline for CountVectorizer & LogisticRegression\n",
    "pipe1 = Pipeline([\n",
    "    ('cvec', CountVectorizer()),\n",
    "    ('lr', LogisticRegression(max_iter=200)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search over the following values of hyperparameters:\n",
    "pipe_params1 = {\n",
    "    'cvec__max_features': [1000, 2000, 3000], # We reduce max_features in order to reduce overfit\n",
    "    'cvec__min_df': [2,3,4,], #  Minimum number of documents needed to include token: 2, 3, 4\n",
    "    'cvec__max_df': [0.9, 0.95], # Maximum number of documents needed to include token: 90%, 95%\n",
    "    'cvec__ngram_range': [(1,1), (1,2)], # search for one-gram and bigram\n",
    "    'lr__C': [1, 0.1, 0.01], # We would like to apply more regularization to reduce overfit,\n",
    "                               # so we do not inlcude large C (small alpha) in search  \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate GridSearchCV.\n",
    "gs1 = GridSearchCV(pipe1, # what object are we optimizing?\n",
    "                  param_grid= pipe_params1, # what parameters values are we searching?\n",
    "                  cv=5) # 5-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('cvec', CountVectorizer()),\n",
       "                                       ('lr',\n",
       "                                        LogisticRegression(max_iter=200))]),\n",
       "             param_grid={'cvec__max_df': [0.9, 0.95],\n",
       "                         'cvec__max_features': [1000, 2000, 3000],\n",
       "                         'cvec__min_df': [2, 3, 4],\n",
       "                         'cvec__ngram_range': [(1, 1), (1, 2)],\n",
       "                         'lr__C': [1, 0.1, 0.01]})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit GridSearch to training data.\n",
    "gs1.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9000682244584685"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What's the best score?\n",
    "gs1.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save best model as gs1_model.\n",
    "gs1_model =gs1.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9909154437456325"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Score model on training set.\n",
    "gs1_model.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9121338912133892"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Score model on testing set.\n",
    "gs1_model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cvec__max_df': 0.9,\n",
       " 'cvec__max_features': 3000,\n",
       " 'cvec__min_df': 2,\n",
       " 'cvec__ngram_range': (1, 2),\n",
       " 'lr__C': 1}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the best marameters\n",
    "gs1.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use Pipeline for TfidfVectorizer Logisctic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate pipeline for TfidfVectorizer & LogisticRegression\n",
    "pipe2 = Pipeline([\n",
    "    ('tvec', TfidfVectorizer()),\n",
    "    ('lr', LogisticRegression(max_iter = 200)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search over the following values of hyperparameters:\n",
    "pipe_params2 = {\n",
    "    'tvec__max_features': [1000, 2000, 3000], # We reduce max_features in order to reduce overfit\n",
    "    'tvec__min_df': [2,3,4],   #Minimum number of documents needed to include token: 2, 3, 4\n",
    "    'tvec__max_df': [0.9, 0.95], # Maximum number of documents needed to include token: 90%, 95%\n",
    "    'tvec__ngram_range': [(1,1), (1,2)], # search for single-gram and bigram\n",
    "    'lr__C': [1, 0.1, 0.01], # We would like to apply more regularization to reduce overfit,\n",
    "                             # so we do not inlcude large C (small alpha) in search  \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate GridSearchCV.\n",
    "gs2 = GridSearchCV(pipe2, # what object are we optimizing?\n",
    "                  param_grid= pipe_params2, # what parameters values are we searching?\n",
    "                  cv=5) # 5-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('tvec', TfidfVectorizer()),\n",
       "                                       ('lr',\n",
       "                                        LogisticRegression(max_iter=200))]),\n",
       "             param_grid={'lr__C': [1, 0.1, 0.01], 'tvec__max_df': [0.9, 0.95],\n",
       "                         'tvec__max_features': [1000, 2000, 3000],\n",
       "                         'tvec__min_df': [2, 3, 4],\n",
       "                         'tvec__ngram_range': [(1, 1), (1, 2)]})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit GridSearch to training data.\n",
    "gs2.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9098389415462587"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What's the best score?\n",
    "gs2.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save best model as gs2_model.\n",
    "gs2_model=gs2.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9720475192173306"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Score model on training set.\n",
    "gs2_model.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9142259414225942"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Score model on test set.\n",
    "gs2_model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lr__C': 1,\n",
       " 'tvec__max_df': 0.9,\n",
       " 'tvec__max_features': 3000,\n",
       " 'tvec__min_df': 4,\n",
       " 'tvec__ngram_range': (1, 2)}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the best marameters\n",
    "gs2.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use Pipeline for CountVectorizer Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate pipeline for CountVectorizer & MultinomialNB\n",
    "pipe3 = Pipeline([\n",
    "    ('cvec', CountVectorizer()),\n",
    "    ('nb', MultinomialNB()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search over the following values of hyperparameters:\n",
    "pipe_params3 = {\n",
    "    'cvec__max_features': [1000, 2000, 3000], # We reduce max_features in order to reduce overfit\n",
    "    'cvec__min_df': [2,3,4], #Minimum number of documents needed to include token: 2, 3, 4\n",
    "    'cvec__max_df': [0.9, 0.95], # Maximum number of documents needed to include token: 90%, 95%\n",
    "    'cvec__ngram_range': [(1,1), (1,2)],  # search for one-gram and bigram\n",
    "    'nb__alpha': [1, 10, 100] # We would like to apply more regularization to reduce overfit,\n",
    "                             # so we do not inlcude small alpha in search\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate GridSearchCV.\n",
    "gs3 = GridSearchCV(pipe3, # what object are we optimizing?\n",
    "                  param_grid= pipe_params3, # what parameters values are we searching?\n",
    "                  cv=5) # 5-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('cvec', CountVectorizer()),\n",
       "                                       ('nb', MultinomialNB())]),\n",
       "             param_grid={'cvec__max_df': [0.9, 0.95],\n",
       "                         'cvec__max_features': [1000, 2000, 3000],\n",
       "                         'cvec__min_df': [2, 3, 4],\n",
       "                         'cvec__ngram_range': [(1, 1), (1, 2)],\n",
       "                         'nb__alpha': [1, 10, 100]})"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit GridSearch to training data.\n",
    "gs3.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9000438585804439"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What's the best score?\n",
    "gs3.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save best model as gs3_model.\n",
    "gs3_model =gs3.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9419986023759609"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Score model on training set.\n",
    "gs3_model.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9037656903765691"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Score model on test set.\n",
    "gs3_model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cvec__max_df': 0.9,\n",
       " 'cvec__max_features': 3000,\n",
       " 'cvec__min_df': 3,\n",
       " 'cvec__ngram_range': (1, 2),\n",
       " 'nb__alpha': 1}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the best parameters\n",
    "gs3.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use Pipeline for TfidfVectorizer Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate pipeline for TfidfVectorizer & MultinomialNB\n",
    "pipe4 = Pipeline([\n",
    "    ('tvec', TfidfVectorizer()),\n",
    "    ('nb', MultinomialNB()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search over the following values of hyperparameters:\n",
    "pipe_params4 = {\n",
    "    'tvec__max_features': [1000, 2000, 3000], # We reduce max_features in order to reduce overfit\n",
    "    'tvec__min_df': [2,3,4], #Minimum number of documents needed to include token: 2, 3, 4\n",
    "    'tvec__max_df': [0.9, 0.95], # Maximum number of documents needed to include token: 90%, 95%\n",
    "    'tvec__ngram_range': [(1,1), (1,2)], # search for single-gram and bigram\n",
    "    'nb__alpha': [1, 10, 100] # We would like to apply more regularization to reduce overfit,\n",
    "                             # so we do not inlcude small alpha in search\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate GridSearchCV.\n",
    "gs4 = GridSearchCV(pipe4, # what object are we optimizing?\n",
    "                  param_grid= pipe_params4, # what parameters values are we searching?\n",
    "                  cv=5) # 5-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('tvec', TfidfVectorizer()),\n",
       "                                       ('nb', MultinomialNB())]),\n",
       "             param_grid={'nb__alpha': [1, 10, 100], 'tvec__max_df': [0.9, 0.95],\n",
       "                         'tvec__max_features': [1000, 2000, 3000],\n",
       "                         'tvec__min_df': [2, 3, 4],\n",
       "                         'tvec__ngram_range': [(1, 1), (1, 2)]})"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit GridSearch to training data\n",
    "gs4.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9014497697424527"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What's the best score?\n",
    "gs4.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save best model as gs4_model.\n",
    "gs4_model =gs4.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9664570230607966"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Score model on training set.\n",
    "gs4_model.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9205020920502092"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Score model on test set.\n",
    "gs4_model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'nb__alpha': 1,\n",
       " 'tvec__max_df': 0.9,\n",
       " 'tvec__max_features': 3000,\n",
       " 'tvec__min_df': 3,\n",
       " 'tvec__ngram_range': (1, 2)}"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the best paramaters\n",
    "gs4.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare and choose best model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Name | Vectorizer |Train Score|Test Score|Train/Test Score gap\n",
    "-|-|-|-|-\n",
    "Logistic Regression|CountVectorizer|99.1%|91.2%|7.9%\n",
    "Logistic Regression|TfidfVectorizer|97.2%|91.4%|5.8%\n",
    "Naive Bayes|CountVectorizer|94.2%|90.4%|3.8%\n",
    "Naive Bayes|TfidfVectorizer|96.6%|92.1%|4.5%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the score table, for logistic regression, with both CountVectorizer and TfidfVectorizer, the gap between train score and test score are quite large, i.e. 5.8-7.9% with highly overfitting.\n",
    "\n",
    "For Naive Bayes, with both CountVectorizer and TfidfVectorizer, the gap between train score and test data is smaller, i.e. 3.8-4.5%. The model still overfits but not too much comparing to logistic regression.\n",
    "\n",
    "Logistic regression is discriminative model which estimated probability(y|x) directly from the training data by minimizing error. \n",
    "Naive Bayes is generative model which estimates a joint probability with features (x) and label y from training data. In another word, generative model can learn from training data. So comparing to Logistic regression, Naive Bayes can get better accuracy score for test data. \n",
    "\n",
    "Among these 4 models, I choose Naive Bayes combined with TfidfVectorizer as best model because this model does not overfit a lot and with better accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation and Conceptual Understanding¶"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Table with best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predict tea</th>\n",
       "      <th>predict coffee</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>actual tea</th>\n",
       "      <td>226</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>actual coffee</th>\n",
       "      <td>18</td>\n",
       "      <td>214</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               predict tea  predict coffee\n",
       "actual tea             226              20\n",
       "actual coffee           18             214"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate a confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_preds = gs4.predict(X_test)\n",
    "\n",
    "pd.DataFrame(confusion_matrix(y_test, y_preds),\n",
    "            columns=['predict tea', 'predict coffee'],\n",
    "            index=['actual tea', 'actual coffee'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9205\n",
      "Misclassification rate: 0.0795\n",
      "Precision: 0.9145\n",
      "Recall: 0.9224\n",
      "Specificity: 0.9187\n"
     ]
    }
   ],
   "source": [
    "# Examine some classification metrics \n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_preds).ravel()\n",
    "print('Accuracy: {}'.format(round((tp+tn)/(tp+fp+tn+fn),4)))\n",
    "print('Misclassification rate: {}'.format(round((fp+fn)/(tp+fp+tn+fn),4)))\n",
    "print('Precision: {}'.format(round(tp/(tp+fp),4)))\n",
    "print('Recall: {}'.format(round(tp/(tp+fn),4)))\n",
    "print('Specificity: {}'.format(round(tn/(tn+fp),4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model correctly predicts 92.05% of observations.                                                                             \n",
    "Among posts that our model predicted to be in /r/Coffee, we have 91.45% of them correctly classified.                           \n",
    "Among posts that are in /r/Coffee, our model has 92.24% of them correctly classified.                                           \n",
    "Among posts that are in /r/Tea, our model has 91.87% of them correctly classified.                                              \n",
    "\n",
    "In our best model, we still have 7.95% of misclassification. We will explore in 5.2 to find out whether any key words to cause misclassification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate misclassification posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preds_prob = gs3.predict_proba(X_test)\n",
    "preds_prob = gs4.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_combine</th>\n",
       "      <th>preds_prob</th>\n",
       "      <th>preds</th>\n",
       "      <th>true_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1292</th>\n",
       "      <td>local store haul thank one client</td>\n",
       "      <td>0.193101</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>947</th>\n",
       "      <td>hello master fellow lover online world far eng...</td>\n",
       "      <td>0.252903</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1703</th>\n",
       "      <td>look large double seal canister expensive hold...</td>\n",
       "      <td>0.465123</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>641</th>\n",
       "      <td>wonder anyone shed light seem like two identic...</td>\n",
       "      <td>0.778849</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>point wife snob thinking agree background cana...</td>\n",
       "      <td>0.510257</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1702</th>\n",
       "      <td>today brew organic green stem twig</td>\n",
       "      <td>0.079966</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1150</th>\n",
       "      <td>know anything three month ago bit problem</td>\n",
       "      <td>0.604524</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>990</th>\n",
       "      <td>guy day ago ask suggestion name come green bra...</td>\n",
       "      <td>0.132194</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1446</th>\n",
       "      <td>think hard work people deserve recognition</td>\n",
       "      <td>0.710618</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>840</th>\n",
       "      <td>hello quit desprete buy calibra melita grinder...</td>\n",
       "      <td>0.834125</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>478 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          clean_combine  preds_prob  preds  \\\n",
       "1292                  local store haul thank one client    0.193101      0   \n",
       "947   hello master fellow lover online world far eng...    0.252903      0   \n",
       "1703  look large double seal canister expensive hold...    0.465123      0   \n",
       "641   wonder anyone shed light seem like two identic...    0.778849      1   \n",
       "105   point wife snob thinking agree background cana...    0.510257      1   \n",
       "...                                                 ...         ...    ...   \n",
       "1702                 today brew organic green stem twig    0.079966      0   \n",
       "1150          know anything three month ago bit problem    0.604524      1   \n",
       "990   guy day ago ask suggestion name come green bra...    0.132194      0   \n",
       "1446         think hard work people deserve recognition    0.710618      1   \n",
       "840   hello quit desprete buy calibra melita grinder...    0.834125      1   \n",
       "\n",
       "      true_y  \n",
       "1292       0  \n",
       "947        0  \n",
       "1703       0  \n",
       "641        1  \n",
       "105        1  \n",
       "...      ...  \n",
       "1702       0  \n",
       "1150       0  \n",
       "990        0  \n",
       "1446       0  \n",
       "840        1  \n",
       "\n",
       "[478 rows x 4 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a dataframe with X_test, preds_prod, preds_y and true_y\n",
    "preds= pd.DataFrame({\n",
    "    'clean_combine':X_test,\n",
    "    'preds_prob':[preds_prob[i][1] for i in range(len(preds_prob))],\n",
    "    'preds': y_preds,\n",
    "    'true_y':y_test \n",
    "})\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column for pred y and true y difference\n",
    "preds['diff'] = preds['preds'] - preds['true_y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "really thinking try know start hope help expertise brew equipment need able brew wide range different temperature control kettle nothing brew bag main category check buy store want get\n",
      "..................................................\n",
      "decide give space\n",
      "..................................................\n",
      "make ceramic water temperature control make ceramic\n",
      "..................................................\n",
      "design little enamel pin witchesvspatriarchy say sub might appreciate\n",
      "..................................................\n",
      "try find decent variable temp kettle easy fancy pay buck ekg small capacity little flat kettle table small capacity normal use aside also gooseneck advantage find linear flow useful kitchen application xiaomi smart kettle window see much water also something like recommend variable temp kettle gooseneck\n",
      "..................................................\n",
      "hey anyone bubble recipe taste like kind make boba shop try make several time really taste idk bubble recipe\n",
      "..................................................\n",
      "know problem seem brew hot seem get deep strong flavor cold brew try play water temperature amount put get strong time overnight brew strong flavor\n",
      "..................................................\n",
      "make jingdezhen\n",
      "..................................................\n",
      "aficionado get one little pot filter build inside like ease boiling water add extract cup many gram would use say day extract use indian ground look loose gifted many day safe extract grind water\n",
      "..................................................\n",
      "use milk alternative favourite use milk alternative\n",
      "..................................................\n",
      "else need right\n",
      "..................................................\n",
      "cold brew\n",
      "..................................................\n",
      "year journey finally space money table chair issue drag outside want gentle breeze\n",
      "..................................................\n",
      "anyone need optimal brewing temperature time\n",
      "..................................................\n",
      "original gift idea fail great backup\n",
      "..................................................\n",
      "hello make french press taste steep use loose leaf black degree water bloom second steep additional minute think may squeeze little much release tannin adjust make less bitter thank advance ratio french press\n",
      "..................................................\n",
      "warren mackenzie yunomi amber glaze unknown creation date purchase year ago\n",
      "..................................................\n",
      "buy haru bancha yuuki cha one recommend vender use gram dry water minute still taste light way make taste strong use gram always taste light way make haru bancha strong\n",
      "..................................................\n",
      "know anything three month ago bit problem\n",
      "..................................................\n",
      "think hard work people deserve recognition\n",
      "..................................................\n"
     ]
    }
   ],
   "source": [
    "# predict is coffee post, but actual is tea post\n",
    "\n",
    "for i in preds.loc[preds['diff'] == 1].clean_combine:\n",
    "    print(i)\n",
    "    print('.'*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Inside these mis-prediction, there are some words, like 'taste', 'brew', etc, which could be a significant term that misclassified them as Coffee. These words are in the top20 words list for Coffee. We also observe the some posts are quite short. These short posts may not contain meaning words, thus cause misclassification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_combine</th>\n",
       "      <th>preds_prob</th>\n",
       "      <th>preds</th>\n",
       "      <th>true_y</th>\n",
       "      <th>diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>323</th>\n",
       "      <td>ice tray would highly recommend make extra pou...</td>\n",
       "      <td>0.464791</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>last year seem clear market high quality seem ...</td>\n",
       "      <td>0.357149</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>howdy love taste however totally caffeine into...</td>\n",
       "      <td>0.437966</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>647</th>\n",
       "      <td>really seem know talk probably miss lot great ...</td>\n",
       "      <td>0.359928</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>big bottle water infuse mint leave lead idea a...</td>\n",
       "      <td>0.264736</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>recently ecuadorian black really like would li...</td>\n",
       "      <td>0.359504</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>little bit context like give background two pl...</td>\n",
       "      <td>0.325214</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>sorta new mocha frappe latte currently drift t...</td>\n",
       "      <td>0.429948</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>784</th>\n",
       "      <td>try blind cup sample pack angel cup see lot pe...</td>\n",
       "      <td>0.392595</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>hello import family farm salvador united state...</td>\n",
       "      <td>0.470261</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799</th>\n",
       "      <td>basically hope find way recreate califia espre...</td>\n",
       "      <td>0.490552</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>anyone know townsend english heritage taste hi...</td>\n",
       "      <td>0.335533</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>615</th>\n",
       "      <td>hello tour latin america currently guatemala l...</td>\n",
       "      <td>0.277755</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>amazon silver wilfa svart worth pick price wil...</td>\n",
       "      <td>0.424082</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>468</th>\n",
       "      <td>recently really great blend natural washed bea...</td>\n",
       "      <td>0.472059</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>458</th>\n",
       "      <td>anyone make syrup home recently really iced co...</td>\n",
       "      <td>0.413336</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>835</th>\n",
       "      <td>decaf mushroom anything else love way taste ho...</td>\n",
       "      <td>0.338178</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421</th>\n",
       "      <td>hello guy look purchase mineral mixture simila...</td>\n",
       "      <td>0.421673</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         clean_combine  preds_prob  preds  \\\n",
       "323  ice tray would highly recommend make extra pou...    0.464791      0   \n",
       "181  last year seem clear market high quality seem ...    0.357149      0   \n",
       "117  howdy love taste however totally caffeine into...    0.437966      0   \n",
       "647  really seem know talk probably miss lot great ...    0.359928      0   \n",
       "178  big bottle water infuse mint leave lead idea a...    0.264736      0   \n",
       "179  recently ecuadorian black really like would li...    0.359504      0   \n",
       "477  little bit context like give background two pl...    0.325214      0   \n",
       "499  sorta new mocha frappe latte currently drift t...    0.429948      0   \n",
       "784  try blind cup sample pack angel cup see lot pe...    0.392595      0   \n",
       "566  hello import family farm salvador united state...    0.470261      0   \n",
       "799  basically hope find way recreate califia espre...    0.490552      0   \n",
       "345  anyone know townsend english heritage taste hi...    0.335533      0   \n",
       "615  hello tour latin america currently guatemala l...    0.277755      0   \n",
       "362  amazon silver wilfa svart worth pick price wil...    0.424082      0   \n",
       "468  recently really great blend natural washed bea...    0.472059      0   \n",
       "458  anyone make syrup home recently really iced co...    0.413336      0   \n",
       "835  decaf mushroom anything else love way taste ho...    0.338178      0   \n",
       "421  hello guy look purchase mineral mixture simila...    0.421673      0   \n",
       "\n",
       "     true_y  diff  \n",
       "323       1    -1  \n",
       "181       1    -1  \n",
       "117       1    -1  \n",
       "647       1    -1  \n",
       "178       1    -1  \n",
       "179       1    -1  \n",
       "477       1    -1  \n",
       "499       1    -1  \n",
       "784       1    -1  \n",
       "566       1    -1  \n",
       "799       1    -1  \n",
       "345       1    -1  \n",
       "615       1    -1  \n",
       "362       1    -1  \n",
       "468       1    -1  \n",
       "458       1    -1  \n",
       "835       1    -1  \n",
       "421       1    -1  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.loc[preds['diff'] == -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ice tray would highly recommend make extra pour excess say ice tray come back later frozen cube perfect add iced without detrement water unique way make iced\n",
      "..................................................\n",
      "last year seem clear market high quality seem people want high quality small viable market high scoring quality get price regular quality market top quality third wave\n",
      "..................................................\n",
      "howdy love taste however totally caffeine intolerant relegate decaf like know anybody wisdom impart good brewing method decaf blend flavorful decaf blend recommend thank decaffeinate tip recommendation\n",
      "..................................................\n",
      "really seem know talk probably miss lot great ignorance favorite ever brazil fazenda rio verde ipanema grand cru apricot get hasbean last year brew use beginner grinder cup one moccamaster without much skill ever truly love taste like pure fruit juice incredible well expensive rare gesha since brew find today natural buy last year develop preconceived notion like naturally process cut way add sugar goal stop add look forward super sweet fruity taste enjoy natural amazing fruit flavor buy one enjoy want fruity fermenty fermenty funky taste kill think black white sweet bloom may good start great natural roaster last year decide order recommend bag send italy enjoy hate natural\n",
      "..................................................\n",
      "big bottle water infuse mint leave lead idea anyone ever try infusion know taste good would happen brew mint infuse water\n",
      "..................................................\n",
      "recently ecuadorian black really like would like buy maker try different type would plain black know proper term would really appreciate recommendation try maker get look maker recommendation\n",
      "..................................................\n",
      "little bit context like give background two plant question like tip advice help recover flourish relevant info zone tropical level humidity despite tree flower produce minimal amount bean currently bean total past two year production like increase picture two tree question story time keep short year ago move apartment complex prior resident leave small dry pot die plant outside back patio decoration figure try revive keep without know type plant time plant approximately tall sparse branch die leave completely cover aphid bug move early fall temperature manage shock plant nearly leave shock fall branch exclude top growth one three plant succumb weather never recover manage save two next year struggle experience good spring growth lose decent amount leave even full branch due winter dry climate inside apartment overall lack knowledge plant first place still know specie day grow seemingly happy recall correctly one tree produce fruit point prompt upsize repot last spring much large gal pot past year completely rocket growth today foot tall maybe flower two february past month hundred bud dozen ripen ready flower within week yay throughout entire process well humidify apartment prevent leaf loss difficult especially winter past week grab humidifier see possibly place humid enough also spray leave mister time day past week despite still start brown leave dry much annoyance otherwise try help promote branch growth seem fine grow upwards base tree really barren funky look due harsh revival anyway main question pinch top growth way help promote branch growth towards base tree basically stalk like try repopulate trunk possible also advice prevent brown tip completely dry leave whatever keep plant humid seem like keep leave completely green unfortunate result location either way hope bloom produce exponentially fruit year would love advice increase branch volume overall width shape tree thank advance appreciate pruning care advice rescue tree help appreciate\n",
      "..................................................\n",
      "sorta new mocha frappe latte currently drift towards home creation stick favorite type buy syrup experiment branch stick like\n",
      "..................................................\n",
      "try blind cup sample pack angel cup see lot people talk angel cup much anymore wonder anyone else subscription past tip advice get angel cup cup flight\n",
      "..................................................\n",
      "hello import family farm salvador united state start import already roast package sell online future import green get roasted package question anyone know need type fda certification anything import already roast package united state would love help seem find anything fda website process need also anyone recommendation freight forwarder someone help custom process since would first time import help advice would much appreciate thank import salvador usa\n",
      "..................................................\n",
      "basically hope find way recreate califia espresso cold brew almond addiction poor try regular cold brew water dilute almond milk dice watery thick creaminess califia taste like cold brew purchase even though use sweeten almond milk sweet enough either califia almost like milkshake read post people make cold brew milk want see anyone could help come idea try cold brewing almond milk\n",
      "..................................................\n",
      "anyone know townsend english heritage taste history know oat milk nut milk modern era use water nut use old way prepare say nut milk forme cury cawdel almaund mylk involve blanch almond likely soak water mix wine add spice case ground dry ginger saffron boiling serve cawdel almaund mylk iiii vii take almaunde blaunche drawe hem wyne erto powdour gyngur sugur colour safroun boile serue forth excerpt forme cury samuel pegge material may protect copyright use nut milk wonder happen let mix bit medieval recipe meet modern era\n",
      "..................................................\n",
      "hello tour latin america currently guatemala look comprehensive map encyclopedia different type grow central south america visit anyone recommend book website find list map format look depth guide available want miss anything thank help look map variety latin america\n",
      "..................................................\n",
      "amazon silver wilfa svart worth pick price wilfa svart cheap\n",
      "..................................................\n",
      "recently really great blend natural washed bean normally fan natural really enjoy play natural wash life remember try take well note log think might barnine nine nouveau hold look recommendation natural washed blend might run particular interesting roaster think particularly good monitor thought natural washed blend totally appreciate thank natural washed blend reco\n",
      "..................................................\n",
      "anyone make syrup home recently really iced coconut syrup buy small shop want start make mine home cheap tip recipe make simple syrup add coconut extract maybe make coconut milk sugar stove plenty recipe online sure one good thank homemade coconut syrup\n",
      "..................................................\n",
      "decaf mushroom anything else love way taste horrible anxiety try cut back difficult also favorite drink hahah usually order decaf starbuck local place sometimes taste like shit without caffeine\n",
      "..................................................\n",
      "hello guy look purchase mineral mixture similar one offer perfect water third wave water live europe want pay euro ship european company supply similar product thank mineral mix water\n",
      "..................................................\n"
     ]
    }
   ],
   "source": [
    "# predict is tea post, but actual is coffee post\n",
    "\n",
    "for i in preds.loc[preds['diff'] == -1].clean_combine:\n",
    "    print(i)\n",
    "    print('.'*50)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Inside these mis-prediction, there are some words, like 'cup', 'brew', etc which could be a significant term that misclassified them as Tea. These words are in the top20 words list for Tea."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion and Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our naive Bayes classifier performed well with a test accuracy score of 92.05%. This is expected due to two similar subreddit topics we chose. There are many common words in their top20 words list, like 'cup', 'brew','taste', 'water' etc.  \n",
    "However, it still can be a good classifier for customer service team to categorize customer feedback with high accuracy. \n",
    "\n",
    "There are still space for improvement for this classifier: \n",
    "- Optimize stop words, for example to identify the common words for these two topics and add into stopwords. \n",
    "- Collect more text-heavy posts for Tea subreddit\n",
    "- May consider to model more than two topics to improve user's satisfaction. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. https://towardsdatascience.com/text-classification-applications-and-use-cases-beab4bfe2e62\n",
    "2. https://towardsdatascience.com/generative-vs-2528de43a836\n",
    "3. https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html\n",
    "4. https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": "4",
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
